1.  **Clustering**:
    
    -   Clustering is an unsupervised learning task where the model groups similar data points together based on their characteristics or proximity.
    -   Common clustering algorithms include K-means clustering, hierarchical clustering, and DBSCAN (Density-Based Spatial Clustering of Applications with Noise).
    -   Evaluation metrics for clustering include silhouette score, cohesion, and separation measures.
2.  **Dimensionality Reduction**:
    
    -   Dimensionality reduction is an unsupervised learning task where the model reduces the number of features in the data while preserving its important structure or information.
    -   Principal Component Analysis (PCA) is a widely used dimensionality reduction technique that transforms the data into a lower-dimensional space.
    -   Other dimensionality reduction methods include t-SNE (t-Distributed Stochastic Neighbor Embedding) and autoencoders.
3.  **Anomaly Detection**:
    
    -   Anomaly detection aims to identify unusual or anomalous instances in the data that differ significantly from the norm.
    -   It is used to detect fraud, network intrusions, manufacturing defects, and other irregular patterns.
    -   Common techniques for anomaly detection include statistical methods, clustering-based approaches, and autoencoders.
4.  **Feature Extraction**:
    
    -   Feature extraction involves transforming raw data into a set of representative features.
    -   It helps to capture the essential information and reduce the dimensionality of the data.
    -   Techniques like Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Singular Value Decomposition (SVD) are used for feature extraction.
5.  **Data Visualization**:
    
    -   Unsupervised learning techniques can be utilized for data visualization and exploratory data analysis.
    -   Visualization methods like scatter plots, heatmaps, and network graphs help gain insights into the underlying structure or patterns in the data.
6.  **Data Preprocessing**:
    
    -   Data preprocessing is crucial in unsupervised learning to handle missing values, normalize data, and address outliers.
    -   Techniques like data cleaning, feature scaling, and handling missing values ensure the quality and reliability of the data.